ğŸ“ Project: APOD Content Classification ğŸŒŒ
ğŸ“– Description

This project builds a content classification model for the APOD (Astronomy Picture of the Day) dataset.
The goal is to predict whether a record is:

ğŸ–¼ï¸ image

ğŸ¥ no-image (merging video and other classes)

The workflow uses:

âœï¸ TF-IDF for text vectorization (title + description)

âš–ï¸ RandomOverSampler for class balancing

ğŸŒ² RandomForest as the classifier

ğŸ“‚ Project Structure
â”œâ”€â”€ notebook.ipynb          # Main notebook with the full workflow
â”œâ”€â”€ data/
â”‚   â””â”€â”€ apod.csv            # Input dataset
â”œâ”€â”€ README.md               # This file
â””â”€â”€ requirements.txt        # Required Python libraries

ğŸ› ï¸ Key Libraries

pandas

scikit-learn (TfidfVectorizer, RandomForestClassifier, train_test_split, classification_report)

imblearn (RandomOverSampler)

matplotlib (for plots, e.g., Precision-Recall Curve)

collections.Counter (for class distribution visualization)

ğŸ”„ Workflow

âœï¸ Text Preprocessing

Concatenate title + description â†’ full_text.

Vectorize using TF-IDF.

âš–ï¸ Class Balancing

Apply oversampling only on the training set using RandomOverSampler.

Avoid contaminating the test set.

ğŸŒ² Model Training

Classifier: RandomForestClassifier with class_weight='balanced'.

Train on the balanced data.

ğŸ“Š Evaluation

Metrics: Precision, Recall, F1-score, Accuracy.

Confusion matrix.

Use predict_proba to adjust the threshold for no-image.

Precision-Recall curve to select the optimal threshold.

ğŸ¯ Expected Results

The model correctly predicts most ğŸ–¼ï¸ image examples.

Adjusting the threshold improves recall for the minority class ğŸ¥ no-image.

Final output includes precision, recall, F1-score, confusion matrix, and the Precision-Recall curve.

ğŸš€ How to Run

Install dependencies:

pip install -r requirements.txt


Run the notebook.ipynb step by step.

Adjust the threshold for ğŸ¥ no-image based on the Precision-Recall curve.

âš ï¸ Notes

Due to extreme class imbalance, it is recommended to merge minority classes or collect more data.

Metrics like Accuracy can be misleading; use macro F1 and balanced accuracy for fair evaluation.
